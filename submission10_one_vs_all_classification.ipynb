{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Step One-vs-All Prediction\n",
    "Score on Driven Data: 0.7879\n",
    "\n",
    "**Summary:** In our research, we found out that there is a strong seperation between non-functional and the two functional classes (functional & functional needs repair). Therefore, this approach starts with a one-vs-all classification to find if the pump is non-functional or not. In the second step, a different model tries to differentiate those pump predicted 'functional', to find if they are 'functional' or 'functional needs repair'. \n",
    "\n",
    "**Content:**\n",
    "1. Data Loading\n",
    "2. Data Cleaning\n",
    "3. One-hot Encoding and Scaling\n",
    "4. Feature Creation\n",
    "    1. Naive Bayes\n",
    "    2. Linear Discriminant Analysis & Logistic Regression\n",
    "    3. Principal Component Analysis & kNN\n",
    "5. Feature Selection\n",
    "6. Model Step 1\n",
    "7. Model Step 2\n",
    "8. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "In our exploratory data analysis, we have identified a number of columns which are clearly irrelevant, that contain duplicate information or that are categorical and contain too many categories to use them. We drop these columns directly after loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_train = pd.read_csv('train_features.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "X_test = pd.read_csv('test_features.csv')\n",
    "y_test = pd.read_csv('submission_format.csv')\n",
    "\n",
    "# merge features and labels on train set\n",
    "train = X_train.copy()\n",
    "train = train.merge(y_train, how = 'left', on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column to always drop\n",
    "columns_to_drop = [\n",
    "    'id',\n",
    "    'subvillage',\n",
    "    'region_code',\n",
    "    'district_code',\n",
    "    'wpt_name',\n",
    "    'recorded_by',\n",
    "    'scheme_name',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'extraction_type_group',\n",
    "    'extraction_type_class',\n",
    "    'waterpoint_type_group',\n",
    "    'quality_group',\n",
    "    'quantity_group',\n",
    "    'source_type',\n",
    "    'source_class',\n",
    "    'num_private', \n",
    "    'date_recorded',\n",
    "    'funder',\n",
    "    'installer',\n",
    "    'lga',\n",
    "    'ward',\n",
    "    'scheme_management'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "X_train.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59400 entries, 0 to 59399\n",
      "Data columns (total 17 columns):\n",
      "amount_tsh           59400 non-null float64\n",
      "gps_height           59400 non-null int64\n",
      "longitude            59400 non-null float64\n",
      "latitude             59400 non-null float64\n",
      "basin                59400 non-null object\n",
      "region               59400 non-null object\n",
      "population           59400 non-null int64\n",
      "public_meeting       56066 non-null object\n",
      "permit               56344 non-null object\n",
      "construction_year    59400 non-null int64\n",
      "extraction_type      59400 non-null object\n",
      "management           59400 non-null object\n",
      "payment_type         59400 non-null object\n",
      "water_quality        59400 non-null object\n",
      "quantity             59400 non-null object\n",
      "source               59400 non-null object\n",
      "waterpoint_type      59400 non-null object\n",
      "dtypes: float64(3), int64(3), object(11)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# show remaining columns\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "**construction_year:** This feature has a large portion of values which are 0. To clean this data, we impute these missing values with the mean construction year. Also, we create a new column to keep the information of whether the construction year was properly recorded or not. \n",
    "\n",
    "**longitude, latitude:** The two geolocation columns also have values which are zero. To impute these values, we impute them with the mean longitude and latitude for the region that the pump is located in. Again, we create a column to store the information of whether the longitude and latitude was recorded or not. \n",
    "\n",
    "**public_meeting, permit:** These two boolean variables contain a few missing values (around 3000, ~5% of the data). We decided to impute them with the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column storing the info whether construction year was recorded or not\n",
    "X_train['construction_year_recorded'] = np.where(X_train.construction_year == 0, False, True)\n",
    "X_test['construction_year_recorded'] = np.where(X_test.construction_year == 0, False, True)\n",
    "\n",
    "# replace construction_year == 0 with the mean construction year\n",
    "mean_construction_year = round(X_train.loc[X_train.construction_year != 0, 'construction_year'].mean(), 0)\n",
    "X_train.loc[X_train.construction_year == 0, 'construction_year'] = mean_construction_year\n",
    "X_test.loc[X_test.construction_year == 0, 'construction_year'] = mean_construction_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column storing the info whether longitude/latitude was recorded or not\n",
    "X_train['longitude_recorded'] = np.where(abs(X_train.longitude) < 0.1, False, True)\n",
    "X_train['latitude_recorded'] = np.where(abs(X_train.latitude) < 0.1, False, True)\n",
    "\n",
    "X_test['longitude_recorded'] = np.where(X_test.longitude < 0.1, False, True)\n",
    "X_test['latitude_recorded'] = np.where(X_test.latitude < 0.1, False, True)\n",
    "\n",
    "# calculate the mean longitude/latitude for each region\n",
    "mean_longitude = [X_train.loc[X_train.region == region,'longitude'].mean() for region in X_train.region.unique()]\n",
    "mean_latitude = [X_train.loc[X_train.region == region,'latitude'].mean() for region in X_train.region.unique()]\n",
    "\n",
    "mean_location = pd.DataFrame(data = {'mean_longitude' : mean_longitude,\n",
    "                                     'mean_latitude' : mean_latitude},\n",
    "                             index = X_train.region.unique())\n",
    "\n",
    "# replace longitudes/latitudes close to 0 with the mean longitude/latitude for the region\n",
    "for i in range(0,X_train.shape[0]):\n",
    "    \n",
    "    # replace longitudes around 0 with the mean for the respective region of the observation\n",
    "    if abs(X_train.loc[i, 'longitude']) < 0.1:\n",
    "        X_train.loc[i, 'longitude'] = mean_location.loc[X_train.loc[i, 'region'], 'mean_longitude']\n",
    "        \n",
    "    # do the same for the latitude\n",
    "    if abs(X_train.loc[i, 'latitude']) < 0.1:\n",
    "        X_train.loc[i, 'latitude'] = mean_location.loc[X_train.loc[i, 'region'], 'mean_latitude']\n",
    "\n",
    "# same for test set\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    \n",
    "    # replace longitudes around 0 with the mean for the respective region of the observation\n",
    "    if abs(X_test.loc[i, 'longitude']) < 0.1:\n",
    "        X_test.loc[i, 'longitude'] = mean_location.loc[X_test.loc[i, 'region'], 'mean_longitude']\n",
    "        \n",
    "    # do the same for the latitude\n",
    "    if abs(X_test.loc[i, 'latitude']) < 0.1:\n",
    "        X_test.loc[i, 'latitude'] = mean_location.loc[X_test.loc[i, 'region'], 'mean_latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values in public_meeting with the majority category (True)\n",
    "X_train.loc[X_train.public_meeting.isna(), 'public_meeting'] = True\n",
    "X_test.loc[X_test.public_meeting.isna(), 'public_meeting'] = True\n",
    "\n",
    "# replace missing values in permit with the majority category (True)\n",
    "X_train.loc[X_train.permit.isna(), 'permit'] = True\n",
    "X_test.loc[X_test.permit.isna(), 'permit'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot Encoding and Scaling\n",
    "All the algorithms we use below (LDA, PCA, Naive Bayes and XGBoost) require numerical data. Therefore, we one-hot encode the categorical data and scale the numerical data using a power transformer. \n",
    "\n",
    "We also map the target to be numerical and create two binary target variables, one for step1 and one for step2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:121: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, \n",
    "                         prefix = X_train.select_dtypes('object').columns, \n",
    "                         columns = X_train.select_dtypes('object').columns,\n",
    "                         drop_first = False\n",
    "                        )\n",
    "\n",
    "X_test = pd.get_dummies(X_test, \n",
    "                         prefix = X_test.select_dtypes('object').columns, \n",
    "                         columns = X_test.select_dtypes('object').columns,\n",
    "                         drop_first = False\n",
    "                        )\n",
    "\n",
    "# power transformation of numerical columns\n",
    "numerical_columns = X_train.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "pt = PowerTransformer()\n",
    "X_train.loc[:,numerical_columns] = pt.fit_transform(X_train.loc[:,numerical_columns])\n",
    "X_test.loc[:,numerical_columns] = pt.transform(X_test.loc[:,numerical_columns])\n",
    "\n",
    "# add columns to test set that only exist in train set\n",
    "X_test[list(set(X_train.columns).difference(set(X_test.columns)))[0]] = 0\n",
    "\n",
    "# make sure columns are in the same order\n",
    "X_train = X_train[sorted(X_train.columns)].copy()\n",
    "X_test = X_test[sorted(X_test.columns)].copy()\n",
    "\n",
    "# convert boolean into numerical\n",
    "for column in X_train.select_dtypes('bool').columns:\n",
    "    X_train[column] = X_train[column].astype(int)\n",
    "    X_test[column] = X_test[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping for the multinomial classes\n",
    "multinomial_classes = {\n",
    "    'functional' : 0,\n",
    "    'non functional' : 1,\n",
    "    'functional needs repair' : 2\n",
    "}\n",
    "\n",
    "# create the inverse mapping\n",
    "classes_inv = {v: k for k, v in multinomial_classes.items()}\n",
    "\n",
    "# map the target to numerical\n",
    "y_train_multinomial = y_train.status_group.map(multinomial_classes).copy()\n",
    "\n",
    "# create binary classes for both steps\n",
    "y_train_binary_step1 = np.where(y_train_multinomial == 1, 1, 0)\n",
    "y_train_binary_step2 = y_train_multinomial[y_train_multinomial != 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_train for step1 and step2\n",
    "X_train_step1 = X_train.copy()\n",
    "X_train_step2 = X_train.loc[y_train_multinomial != 1,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Creation\n",
    "To add more information to the dataset, we use different classifiers to create a new feature with the probability that an each observation is of class 0 or class 1. We use three approaches here:\n",
    "\n",
    "**Naive Bayes:** We train a naive bayes classifier on only the one-hot encoded categorical features. It's accuracy is 0.76.\n",
    "\n",
    "**LDA & Logistic Regression:** We first use a linear discriminant analysis (LDA) to get the dimension of biggest variance between the two classes in step1. Since LDA doesn't allow us to predict probabilities, we train a logistic regression on that single feature. It's accuracy is 0.79.\n",
    "\n",
    "**PCA & kNN:** We use PCA with the first 10 principal components to create a new feature space with a reduced number of features. This allows us to train a kNN model without having the curse of dimensionality that we would have with the over 100 dimensions of the train set. We use 21 neighbors, because our research found the the number of neighbors doesn't have a large impact on predictive power and with a larger number of neighbors we have lower variance and it allows use to get more distint probabilities when using the predict_proba function. This kNN model has an accuracy of 0.82.\n",
    "\n",
    "We only used this approach for step1, because we saw that for step2, these models didn't have any predictive power. In step2, the class ratio is 0.88 to 0.12 ('functional' to 'functional needs repair'), and all these models didn't score higher than 0.88.\n",
    "\n",
    "### A. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes step1 cross-validation: [0.76212121 0.75294613 0.76498316 0.76077441 0.76599327]\n"
     ]
    }
   ],
   "source": [
    "# take only the one-hot encoded categorical features\n",
    "nb_features = X_train_step1.select_dtypes('uint8').columns\n",
    "\n",
    "# create and fit a naive bayes classifier for step1, print cross-validation score for reference\n",
    "nb_step1 = GaussianNB()\n",
    "\n",
    "step1_cross_val_score = cross_val_score(nb_step1, X_train_step1[nb_features], y_train_binary_step1)\n",
    "print(f'Naive Bayes step1 cross-validation: {step1_cross_val_score}')\n",
    "\n",
    "nb_step1.fit(X_train_step1[nb_features], y_train_binary_step1)\n",
    "\n",
    "# predict probabilities\n",
    "X_train_step1['naive_bayes_proba'] = nb_step1.predict_proba(X_train_step1[nb_features])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Linear Discriminant Analysis & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA & Logit Model step1 cross-validation: [0.79267677 0.78905724 0.78897306 0.78728956 0.7989899 ]\n"
     ]
    }
   ],
   "source": [
    "# create and fit an LDA model\n",
    "lda_step1 = LinearDiscriminantAnalysis(solver = 'svd')\n",
    "lda_step1.fit(X_train_step1, y_train_binary_step1)\n",
    "\n",
    "# transform train set\n",
    "lda_train_step1 = lda_step1.transform(X_train_step1)\n",
    "\n",
    "# create and fit a logistic regression model, print cross-validation score for reference\n",
    "logit_step1 = LogisticRegression(penalty = 'none', solver = 'lbfgs')\n",
    "\n",
    "step1_cross_val_score = cross_val_score(logit_step1, lda_train_step1, y_train_binary_step1)\n",
    "print(f'LDA & Logit Model step1 cross-validation: {step1_cross_val_score}')\n",
    "\n",
    "logit_step1.fit(lda_train_step1, y_train_binary_step1)\n",
    "\n",
    "# predict probabilities\n",
    "X_train_step1['logit_proba'] = logit_step1.predict_proba(lda_train_step1)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis & kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA & kNN Model step1 cross-validation: [0.82626263 0.81801347 0.81675084 0.81582492 0.81700337]\n"
     ]
    }
   ],
   "source": [
    "# create and fit a PCA model\n",
    "pca_step1 = PCA(n_components = 10)\n",
    "pca_step1.fit(X_train_step1)\n",
    "\n",
    "# transform data\n",
    "pca_train_step1 = pca_step1.transform(X_train_step1)\n",
    "\n",
    "# create and fit a kNN model\n",
    "knn_step1 = KNeighborsClassifier(n_neighbors = 21)\n",
    "\n",
    "step1_cross_val_score = cross_val_score(knn_step1, pca_train_step1, y_train_binary_step1)\n",
    "print(f'PCA & kNN Model step1 cross-validation: {step1_cross_val_score}')\n",
    "\n",
    "knn_step1.fit(pca_train_step1, y_train_binary_step1)\n",
    "\n",
    "# predict probabilities\n",
    "X_train_step1['knn_proba'] = knn_step1.predict_proba(pca_train_step1)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "For feature selection, we use a method called **permutation importance**. In this method, we train a classifier on a train set. Then the permutation importance algorithm makes predictions on a validation set for a base model. Then, it makes predictions again, this time with the i-th column being randomized . If this randomization of the i-th column leads to a drop in accuracy score, then we know that this feature is relevant. We decide to keep all features where the randomization leads to a drop in accuracy of at least 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110                knn_proba\n",
      "109              logit_proba\n",
      "108        naive_bayes_proba\n",
      "33                 longitude\n",
      "31                  latitude\n",
      "57              quantity_dry\n",
      "107    waterpoint_type_other\n",
      "30                gps_height\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# create a train-validation split for feature selection\n",
    "X_train_step1_split, X_validation_step1_split, y_train_binary_step1_split, y_validation_binary_step1_split \\\n",
    "    = train_test_split(X_train_step1,\n",
    "                       y_train_binary_step1,\n",
    "                       test_size = 0.2,\n",
    "                       stratify = y_train_binary_step1,\n",
    "                       random_state = 12)\n",
    "\n",
    "# create xgboost model\n",
    "xgb_model = xgb.XGBClassifier(max_depth = 12,\n",
    "                              n_estimators = 100,\n",
    "                              learning_rate = 0.1,\n",
    "                              gamma = 0.001,\n",
    "                              booster = 'gbtree',\n",
    "                              min_child_weight = 20,\n",
    "                              subsample = 0.8, \n",
    "                              colsample_bytree = 0.8,\n",
    "                              colsample_bylevel = 0.8,\n",
    "                              colsample_bynode = 0.8,\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 27)\n",
    "\n",
    "xgb_model.fit(X_train_step1_split, y_train_binary_step1_split)\n",
    "\n",
    "# calculate permutation_importance\n",
    "permutation_importance_step1 = permutation_importance(xgb_model, \n",
    "                                                      X_validation_step1_split, \n",
    "                                                      y_validation_binary_step1_split,\n",
    "                                                      n_repeats = 20,\n",
    "                                                      n_jobs = -1,\n",
    "                                                      random_state = 62)\n",
    "\n",
    "permutation_importance_df_step1 = pd.DataFrame({'feature' : X_train_step1_split.columns,\n",
    "                                                'permutation_importance' : permutation_importance_step1['importances_mean']})\n",
    "\n",
    "permutation_importance_df_step1.sort_values(by = ['permutation_importance'], ascending = False, inplace = True)\n",
    "\n",
    "# select those features with a permutation importance > 0.001\n",
    "relevant_features_step1 = permutation_importance_df_step1.loc[permutation_importance_df_step1.permutation_importance > 0.001, \n",
    "                                                              'feature']\n",
    "print(relevant_features_step1)\n",
    "\n",
    "# drop irrelevant features\n",
    "X_train_step1 = X_train_step1[relevant_features_step1].copy()\n",
    "\n",
    "# remove splitted data as it's not needed anymore\n",
    "del X_train_step1_split, X_validation_step1_split, y_train_binary_step1_split, y_validation_binary_step1_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33                                       longitude\n",
      "103    waterpoint_type_communal standpipe multiple\n",
      "31                                        latitude\n",
      "10                               construction_year\n",
      "15                         extraction_type_gravity\n",
      "105                      waterpoint_type_hand pump\n",
      "3                                 basin_Lake Rukwa\n",
      "58                                 quantity_enough\n",
      "56                                  public_meeting\n",
      "53                            payment_type_unknown\n",
      "107                          waterpoint_type_other\n",
      "30                                      gps_height\n",
      "49                          payment_type_never pay\n",
      "65                                   region_Iringa\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# create a train-validation split for feature selection\n",
    "X_train_step2_split, X_validation_step2_split, y_train_binary_step2_split, y_validation_binary_step2_split \\\n",
    "    = train_test_split(X_train_step2,\n",
    "                       y_train_binary_step2,\n",
    "                       test_size = 0.2,\n",
    "                       stratify = y_train_binary_step2,\n",
    "                       random_state = 12)\n",
    "\n",
    "# create xgboost model\n",
    "xgb_model = xgb.XGBClassifier(max_depth = 12,\n",
    "                              n_estimators = 100,\n",
    "                              learning_rate = 0.1,\n",
    "                              gamma = 0.001,\n",
    "                              booster = 'gbtree',\n",
    "                              min_child_weight = 20,\n",
    "                              subsample = 0.8, \n",
    "                              colsample_bytree = 0.8,\n",
    "                              colsample_bylevel = 0.8,\n",
    "                              colsample_bynode = 0.8,\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 27)\n",
    "\n",
    "xgb_model.fit(X_train_step2_split, y_train_binary_step2_split)\n",
    "\n",
    "# calculate permutation_importance\n",
    "permutation_importance_step2 = permutation_importance(xgb_model, \n",
    "                                                      X_validation_step2_split, \n",
    "                                                      y_validation_binary_step2_split,\n",
    "                                                      n_repeats = 20,\n",
    "                                                      n_jobs = -1,\n",
    "                                                      random_state = 62)\n",
    "\n",
    "permutation_importance_df_step2 = pd.DataFrame({'feature' : X_train_step2_split.columns,\n",
    "                                                'permutation_importance' : permutation_importance_step2['importances_mean']})\n",
    "\n",
    "permutation_importance_df_step2.sort_values(by = ['permutation_importance'], ascending = False, inplace = True)\n",
    "\n",
    "# select those features with a permutation importance > 0.001\n",
    "relevant_features_step2 = permutation_importance_df_step2.loc[permutation_importance_df_step2.permutation_importance > 0.001, \n",
    "                                                              'feature']\n",
    "print(relevant_features_step2)\n",
    "\n",
    "# drop irrelevant features\n",
    "X_train_step2 = X_train_step2[relevant_features_step2].copy()\n",
    "\n",
    "# remove splitted data as it's not needed anymore\n",
    "del X_train_step2_split, X_validation_step2_split, y_train_binary_step2_split, y_validation_binary_step2_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Step 1\n",
    "Since XGBoost takes a long time to fit, we can only do a grid-search on a few parameters. However, playing around with parameters in other notebooks has shown that max_depth and min_child_weight are the most impactful to both increase model performance and to avoid overfitting. \n",
    "\n",
    "Decreasing the 'learning_rate' makes the model perform worse. Changing 'gamma' has had absolutely no impact on model performance. Decreasing 'subsample' reduces model overfitting, but also at a large expense to test score. \n",
    "\n",
    "In step 1, we predict if an observation is 'functional', 'functional needs repair' (class 0) or 'non_functional' (class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   44.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 20}</td>\n",
       "      <td>0.872934</td>\n",
       "      <td>0.848805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 20}</td>\n",
       "      <td>0.867551</td>\n",
       "      <td>0.848569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 40}</td>\n",
       "      <td>0.864268</td>\n",
       "      <td>0.847576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 40}</td>\n",
       "      <td>0.861090</td>\n",
       "      <td>0.846970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      params  mean_train_score  \\\n",
       "2  {'max_depth': 12, 'min_child_weight': 20}          0.872934   \n",
       "0  {'max_depth': 10, 'min_child_weight': 20}          0.867551   \n",
       "3  {'max_depth': 12, 'min_child_weight': 40}          0.864268   \n",
       "1  {'max_depth': 10, 'min_child_weight': 40}          0.861090   \n",
       "\n",
       "   mean_test_score  \n",
       "2         0.848805  \n",
       "0         0.848569  \n",
       "3         0.847576  \n",
       "1         0.846970  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameter grid\n",
    "params = {\n",
    "    'max_depth' : [10, 12],\n",
    "    'min_child_weight' : [20, 40]\n",
    "}\n",
    "\n",
    "# create xgboost model\n",
    "xgb_model = xgb.XGBClassifier(learning_rate = 0.1,\n",
    "                              gamma = 0.001,\n",
    "                              subsample = 0.8, \n",
    "                              colsample_bytree = 0.8,\n",
    "                              colsample_bylevel = 0.8,\n",
    "                              colsample_bynode = 0.8,\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 27)\n",
    "\n",
    "# create grid search object\n",
    "grid_xgb_step1 = GridSearchCV(estimator = xgb_model, \n",
    "                              param_grid = params, \n",
    "                              scoring='accuracy',\n",
    "                              n_jobs=-1,\n",
    "                              cv=5,\n",
    "                              refit = True,\n",
    "                              return_train_score = True,\n",
    "                              verbose = 1)\n",
    "\n",
    "# fit the model\n",
    "grid_xgb_step1.fit(X_train_step1, y_train_binary_step1)\n",
    "\n",
    "# read results of grid search into dataframe\n",
    "cv_results_df_step1 = pd.DataFrame(grid_xgb_step1.cv_results_)\n",
    "\n",
    "# print results\n",
    "cv_results_df_step1[['params', 'mean_train_score', 'mean_test_score']].sort_values(by = ['mean_test_score'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Step 2\n",
    "In this step, out of all observations that were classified class 0 ('functional' or 'functional needs repair') are no classified to be 'functional' (class 0 in step 2) or 'functional needs repair' (class 2 in step 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 20}</td>\n",
       "      <td>0.907221</td>\n",
       "      <td>0.900044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 20}</td>\n",
       "      <td>0.905519</td>\n",
       "      <td>0.899634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 40}</td>\n",
       "      <td>0.901999</td>\n",
       "      <td>0.898075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 40}</td>\n",
       "      <td>0.900898</td>\n",
       "      <td>0.897802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      params  mean_train_score  \\\n",
       "2  {'max_depth': 12, 'min_child_weight': 20}          0.907221   \n",
       "0  {'max_depth': 10, 'min_child_weight': 20}          0.905519   \n",
       "3  {'max_depth': 12, 'min_child_weight': 40}          0.901999   \n",
       "1  {'max_depth': 10, 'min_child_weight': 40}          0.900898   \n",
       "\n",
       "   mean_test_score  \n",
       "2         0.900044  \n",
       "0         0.899634  \n",
       "3         0.898075  \n",
       "1         0.897802  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameter grid\n",
    "params = {\n",
    "    'max_depth' : [10, 12],\n",
    "    'min_child_weight' : [20, 40]\n",
    "}\n",
    "\n",
    "# create xgboost model\n",
    "xgb_model = xgb.XGBClassifier(learning_rate = 0.1,\n",
    "                              gamma = 0.001,\n",
    "                              subsample = 0.8, \n",
    "                              colsample_bytree = 0.8,\n",
    "                              colsample_bylevel = 0.8,\n",
    "                              colsample_bynode = 0.8,\n",
    "                              scale_pos_weight = 1,\n",
    "                              seed = 27)\n",
    "\n",
    "# create grid search object\n",
    "grid_xgb_step2 = GridSearchCV(estimator = xgb_model, \n",
    "                              param_grid = params, \n",
    "                              scoring='accuracy',\n",
    "                              n_jobs=-1,\n",
    "                              cv=5,\n",
    "                              refit = True,\n",
    "                              return_train_score = True,\n",
    "                              verbose = 1)\n",
    "\n",
    "# fit the model\n",
    "grid_xgb_step2.fit(X_train_step2, y_train_binary_step2)\n",
    "\n",
    "# read results of grid search into dataframe\n",
    "cv_results_df_step2 = pd.DataFrame(grid_xgb_step2.cv_results_)\n",
    "\n",
    "# print results\n",
    "cv_results_df_step2[['params', 'mean_train_score', 'mean_test_score']].sort_values(by = ['mean_test_score'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction\n",
    "We apply all feature creation here to the test set and the run the two models trained above. After that we map back to the alphanumerical description of the classes (0 -> 'functional', 1 -> 'non_functional', 2 -> 'functional needs repair'), as per the requirements of the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new features to test set\n",
    "X_test['naive_bayes_proba'] = nb_step1.predict_proba(X_test[nb_features])[:,1]\n",
    "\n",
    "lda_test = lda_step1.transform(X_test)\n",
    "X_test['logit_proba'] = logit_step1.predict_proba(lda_test)[:,1]\n",
    "\n",
    "pca_test = pca_step1.transform(X_test)\n",
    "X_test['knn_proba'] = knn_step1.predict_proba(pca_test)[:,1]\n",
    "\n",
    "# Step 1\n",
    "y_pred_step1 = grid_xgb_step1.predict(X_test[relevant_features_step1])\n",
    "\n",
    "# Step 2\n",
    "y_pred_step2 = grid_xgb_step2.predict(X_test.loc[y_pred_step1 == 0, relevant_features_step2])\n",
    "\n",
    "# combine results\n",
    "y_pred = y_pred_step1.copy()\n",
    "y_pred[y_pred_step1 == 0] = y_pred_step2\n",
    "\n",
    "# map back to string classes\n",
    "y_pred = pd.Series(y_pred).map(classes_inv)\n",
    "\n",
    "# create submission data frame\n",
    "y_test.loc[:,'status_group'] = y_pred\n",
    "\n",
    "# write to csv\n",
    "y_test.to_csv('submission10.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 9475\n",
       "non functional             5036\n",
       "functional needs repair     339\n",
       "Name: status_group, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check counts of predicted classes to make sure the predictions make sense\n",
    "y_test.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
