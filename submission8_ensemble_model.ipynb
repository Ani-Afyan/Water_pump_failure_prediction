{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_train = pd.read_csv('train_features.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "X_test = pd.read_csv('test_features.csv')\n",
    "y_test = pd.read_csv('submission_format.csv')\n",
    "\n",
    "# merge features and labels on train set\n",
    "train = X_train.copy()\n",
    "train = train.merge(y_train, how = 'left', on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column to always drop\n",
    "columns_to_drop = [\n",
    "    'id',\n",
    "    'subvillage',\n",
    "    'region_code',\n",
    "    'district_code',\n",
    "    'wpt_name',\n",
    "    'recorded_by',\n",
    "    'scheme_name',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'extraction_type_group',\n",
    "    'extraction_type_class',\n",
    "    'waterpoint_type_group',\n",
    "    'quality_group',\n",
    "    'quantity_group',\n",
    "    'source_type',\n",
    "    'source_class',\n",
    "    'num_private', \n",
    "    'date_recorded',\n",
    "    'scheme_management',\n",
    "    'ward'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "X_train.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59400 entries, 0 to 59399\n",
      "Data columns (total 20 columns):\n",
      "amount_tsh           59400 non-null float64\n",
      "funder               55765 non-null object\n",
      "gps_height           59400 non-null int64\n",
      "installer            55745 non-null object\n",
      "longitude            59400 non-null float64\n",
      "latitude             59400 non-null float64\n",
      "basin                59400 non-null object\n",
      "region               59400 non-null object\n",
      "lga                  59400 non-null object\n",
      "population           59400 non-null int64\n",
      "public_meeting       56066 non-null object\n",
      "permit               56344 non-null object\n",
      "construction_year    59400 non-null int64\n",
      "extraction_type      59400 non-null object\n",
      "management           59400 non-null object\n",
      "payment_type         59400 non-null object\n",
      "water_quality        59400 non-null object\n",
      "quantity             59400 non-null object\n",
      "source               59400 non-null object\n",
      "waterpoint_type      59400 non-null object\n",
      "dtypes: float64(3), int64(3), object(14)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# show remaining columns\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column storing the info whether construction year was recorded or not\n",
    "X_train['construction_year_recorded'] = np.where(X_train.construction_year == 0, False, True)\n",
    "X_test['construction_year_recorded'] = np.where(X_test.construction_year == 0, False, True)\n",
    "\n",
    "# replace construction_year == 0 with the mean construction year\n",
    "mean_construction_year = round(X_train.loc[X_train.construction_year != 0, 'construction_year'].mean(), 0)\n",
    "X_train.loc[X_train.construction_year == 0, 'construction_year'] = mean_construction_year\n",
    "X_test.loc[X_test.construction_year == 0, 'construction_year'] = mean_construction_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column storing the info whether longitude/latitude was recorded or not\n",
    "X_train['longitude_recorded'] = np.where(abs(X_train.longitude) < 0.1, False, True)\n",
    "X_train['latitude_recorded'] = np.where(abs(X_train.latitude) < 0.1, False, True)\n",
    "\n",
    "X_test['longitude_recorded'] = np.where(X_test.longitude < 0.1, False, True)\n",
    "X_test['latitude_recorded'] = np.where(X_test.latitude < 0.1, False, True)\n",
    "\n",
    "# calculate the mean longitude/latitude for each region\n",
    "mean_longitude = [X_train.loc[X_train.region == region,'longitude'].mean() for region in X_train.region.unique()]\n",
    "mean_latitude = [X_train.loc[X_train.region == region,'latitude'].mean() for region in X_train.region.unique()]\n",
    "\n",
    "mean_location = pd.DataFrame(data = {'mean_longitude' : mean_longitude,\n",
    "                                     'mean_latitude' : mean_latitude},\n",
    "                             index = X_train.region.unique())\n",
    "\n",
    "# replace longitudes/latitudes close to 0 with the mean longitude/latitude for the region\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    \n",
    "    # replace longitudes around 0 with the mean for the respective region of the observation\n",
    "    if abs(X_train.loc[i, 'longitude']) < 0.1:\n",
    "        X_train.loc[i, 'longitude'] = mean_location.loc[X_train.loc[i, 'region'], 'mean_longitude']\n",
    "        \n",
    "    # do the same for the latitude\n",
    "    if abs(X_train.loc[i, 'latitude']) < 0.1:\n",
    "        X_train.loc[i, 'latitude'] = mean_location.loc[X_train.loc[i, 'region'], 'mean_latitude']\n",
    "\n",
    "# same for test set\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    \n",
    "    # replace longitudes around 0 with the mean for the respective region of the observation\n",
    "    if abs(X_test.loc[i, 'longitude']) < 0.1:\n",
    "        X_test.loc[i, 'longitude'] = mean_location.loc[X_test.loc[i, 'region'], 'mean_longitude']\n",
    "        \n",
    "    # do the same for the latitude\n",
    "    if abs(X_test.loc[i, 'latitude']) < 0.1:\n",
    "        X_test.loc[i, 'latitude'] = mean_location.loc[X_test.loc[i, 'region'], 'mean_latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values in public_meeting with the majority category (True)\n",
    "X_train.loc[X_train.public_meeting.isna(), 'public_meeting'] = True\n",
    "X_test.loc[X_test.public_meeting.isna(), 'public_meeting'] = True\n",
    "\n",
    "# replace missing values in permit with the majority category (True)\n",
    "X_train.loc[X_train.permit.isna(), 'permit'] = True\n",
    "X_test.loc[X_test.permit.isna(), 'permit'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping for the multinomial classes\n",
    "multinomial_classes = {\n",
    "    'functional' : 0,\n",
    "    'non functional' : 1,\n",
    "    'functional needs repair' : 2\n",
    "}\n",
    "\n",
    "# create the inverse mapping\n",
    "classes_inv = {v: k for k, v in multinomial_classes.items()}\n",
    "\n",
    "# map the target to numerical\n",
    "y_train_multinomial = y_train.status_group.map(multinomial_classes).copy()\n",
    "\n",
    "# create binary classes for functional versus non-function\n",
    "y_train_binary = np.where(y_train_multinomial == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create series of feature 'lga'\n",
    "lgas_train = X_train.lga.copy()\n",
    "lgas_test = X_test.lga.copy()\n",
    "\n",
    "# create value count for lga\n",
    "lga_counts = lgas_train.value_counts()\n",
    "\n",
    "# create a mask filtering value counts lower than 200\n",
    "mask_train = lgas_train.isin(lga_counts[lga_counts > 200].index)\n",
    "mask_test = lgas_test.isin(lga_counts[lga_counts > 200].index)\n",
    "\n",
    "# replace values\n",
    "lgas_train[~mask_train] = 'Other'\n",
    "lgas_test[~mask_test] = 'Other'\n",
    "\n",
    "# dummy encode\n",
    "lgas_train_dummy = pd.get_dummies(lgas_train)\n",
    "lgas_test_dummy = pd.get_dummies(lgas_test)\n",
    "\n",
    "# create and fit naive bayes model\n",
    "nb_lga = GaussianNB()\n",
    "nb_lga.fit(lgas_train_dummy, y_train_binary)\n",
    "\n",
    "# create predictions and add them as new feature\n",
    "X_train['lga'] = nb_lga.predict_proba(lgas_train_dummy)[:,1]\n",
    "X_test['lga'] = nb_lga.predict_proba(lgas_test_dummy)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series of feature 'installer'\n",
    "installers_train = X_train.installer.copy()\n",
    "installers_test = X_test.installer.copy()\n",
    "\n",
    "# create value count for installer\n",
    "installer_counts = installers_train.value_counts()\n",
    "\n",
    "# create a mask filtering value counts lower than 100\n",
    "mask_train = installers_train.isin(installer_counts[installer_counts > 100].index)\n",
    "mask_test = installers_test.isin(installer_counts[installer_counts > 100].index)\n",
    "\n",
    "# replace values\n",
    "installers_train[~mask_train] = 'Other'\n",
    "installers_test[~mask_test] = 'Other'\n",
    "\n",
    "# dummy encode\n",
    "installers_train_dummy = pd.get_dummies(installers_train)\n",
    "installers_test_dummy = pd.get_dummies(installers_test)\n",
    "\n",
    "# create and fit naive bayes model\n",
    "nb_installer = GaussianNB()\n",
    "nb_installer.fit(installers_train_dummy, y_train_binary)\n",
    "\n",
    "# create predictions and add them as new feature\n",
    "X_train['installer'] = nb_installer.predict_proba(installers_train_dummy)[:,1]\n",
    "X_test['installer'] = nb_installer.predict_proba(installers_test_dummy)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series of feature 'funder'\n",
    "funders_train = X_train.funder.copy()\n",
    "funders_test = X_test.installer.copy()\n",
    "\n",
    "# create value count for funder\n",
    "funder_counts = funders_train.value_counts()\n",
    "\n",
    "# create a mask filtering value counts lower than 200\n",
    "mask_train = funders_train.isin(funder_counts[funder_counts > 50].index)\n",
    "mask_test = funders_test.isin(funder_counts[funder_counts > 50].index)\n",
    "\n",
    "# replace values\n",
    "funders_train[~mask_train] = 'Other'\n",
    "funders_test[~mask_test] = 'Other'\n",
    "\n",
    "# dummy encode\n",
    "funders_train_dummy = pd.get_dummies(funders_train)\n",
    "funders_test_dummy = pd.get_dummies(funders_test)\n",
    "\n",
    "# create and fit naive bayes model\n",
    "nb_funders = GaussianNB()\n",
    "nb_funders.fit(funders_train_dummy, y_train_binary)\n",
    "\n",
    "# create predictions and add them as new feature\n",
    "X_train['funder'] = nb_installer.predict_proba(installers_train_dummy)[:,1]\n",
    "X_test['funder'] = nb_installer.predict_proba(installers_test_dummy)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, \n",
    "                         prefix = X_train.select_dtypes('object').columns, \n",
    "                         columns = X_train.select_dtypes('object').columns,\n",
    "                         drop_first = False\n",
    "                        )\n",
    "\n",
    "X_test = pd.get_dummies(X_test, \n",
    "                         prefix = X_test.select_dtypes('object').columns, \n",
    "                         columns = X_test.select_dtypes('object').columns,\n",
    "                         drop_first = False\n",
    "                        )\n",
    "\n",
    "# add columns to test set that only exist in train set\n",
    "X_test[list(set(X_train.columns).difference(set(X_test.columns)))[0]] = 0\n",
    "\n",
    "# make sure columns are in the same order\n",
    "X_train = X_train[sorted(X_train.columns)].copy()\n",
    "X_test = X_test[sorted(X_test.columns)].copy()\n",
    "\n",
    "# convert boolean into numerical\n",
    "for column in X_train.select_dtypes('bool').columns:\n",
    "    X_train[column] = X_train[column].astype(int)\n",
    "    X_test[column] = X_test[column].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit an LDA model\n",
    "lda = LinearDiscriminantAnalysis(solver = 'svd')\n",
    "lda.fit(X_train, y_train_multinomial)\n",
    "\n",
    "# transform X_train and X_test\n",
    "lda_train = lda.transform(X_train)\n",
    "lda_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add linear discriminants to the train and test set\n",
    "X_train['LD1'] = lda_train[:,0]\n",
    "X_train['LD2'] = lda_train[:,1]\n",
    "X_test['LD1'] = lda_test[:,0]\n",
    "X_test['LD2'] = lda_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit a kNN model\n",
    "knn = KNeighborsClassifier(n_neighbors = 21)\n",
    "knn.fit(X_train[['LD1', 'LD2']], y_train_multinomial)\n",
    "\n",
    "# predict probabilities\n",
    "X_train['knn_proba_0'] = knn.predict_proba(X_train[['LD1', 'LD2']])[:,0]\n",
    "X_train['knn_proba_1'] = knn.predict_proba(X_train[['LD1', 'LD2']])[:,1]\n",
    "X_train['knn_proba_2'] = knn.predict_proba(X_train[['LD1', 'LD2']])[:,2]\n",
    "\n",
    "X_test['knn_proba_0'] = knn.predict_proba(X_test[['LD1', 'LD2']])[:,0]\n",
    "X_test['knn_proba_1'] = knn.predict_proba(X_test[['LD1', 'LD2']])[:,1]\n",
    "X_test['knn_proba_2'] = knn.predict_proba(X_test[['LD1', 'LD2']])[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.913102</td>\n",
       "      <td>0.805825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.898005</td>\n",
       "      <td>0.805320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.887327</td>\n",
       "      <td>0.805269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.887201</td>\n",
       "      <td>0.805135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.875484</td>\n",
       "      <td>0.805067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.883392</td>\n",
       "      <td>0.804764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.804529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.908603</td>\n",
       "      <td>0.804461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.893173</td>\n",
       "      <td>0.804394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.867689</td>\n",
       "      <td>0.804209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.902437</td>\n",
       "      <td>0.804007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.879255</td>\n",
       "      <td>0.803923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.876763</td>\n",
       "      <td>0.803485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 14, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.882054</td>\n",
       "      <td>0.803266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.855829</td>\n",
       "      <td>0.802744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.863649</td>\n",
       "      <td>0.802626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.872029</td>\n",
       "      <td>0.802593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.860513</td>\n",
       "      <td>0.802559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.848986</td>\n",
       "      <td>0.802239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>{'max_depth': 12, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.867466</td>\n",
       "      <td>0.802088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.852378</td>\n",
       "      <td>0.802054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 3, 'subs...</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.801768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.845762</td>\n",
       "      <td>0.801717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.844651</td>\n",
       "      <td>0.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.842428</td>\n",
       "      <td>0.800808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 5, 'subs...</td>\n",
       "      <td>0.847058</td>\n",
       "      <td>0.800724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 10, 'min_child_weight': 7, 'subs...</td>\n",
       "      <td>0.840842</td>\n",
       "      <td>0.799764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_train_score  \\\n",
       "20  {'max_depth': 14, 'min_child_weight': 3, 'subs...          0.913102   \n",
       "23  {'max_depth': 14, 'min_child_weight': 5, 'subs...          0.898005   \n",
       "21  {'max_depth': 14, 'min_child_weight': 5, 'subs...          0.887327   \n",
       "11  {'max_depth': 12, 'min_child_weight': 3, 'subs...          0.887201   \n",
       "14  {'max_depth': 12, 'min_child_weight': 5, 'subs...          0.875484   \n",
       "10  {'max_depth': 12, 'min_child_weight': 3, 'subs...          0.883392   \n",
       "26  {'max_depth': 14, 'min_child_weight': 7, 'subs...          0.887189   \n",
       "19  {'max_depth': 14, 'min_child_weight': 3, 'subs...          0.908603   \n",
       "22  {'max_depth': 14, 'min_child_weight': 5, 'subs...          0.893173   \n",
       "17  {'max_depth': 12, 'min_child_weight': 7, 'subs...          0.867689   \n",
       "18  {'max_depth': 14, 'min_child_weight': 3, 'subs...          0.902437   \n",
       "9   {'max_depth': 12, 'min_child_weight': 3, 'subs...          0.879255   \n",
       "24  {'max_depth': 14, 'min_child_weight': 7, 'subs...          0.876763   \n",
       "25  {'max_depth': 14, 'min_child_weight': 7, 'subs...          0.882054   \n",
       "2   {'max_depth': 10, 'min_child_weight': 3, 'subs...          0.855829   \n",
       "16  {'max_depth': 12, 'min_child_weight': 7, 'subs...          0.863649   \n",
       "13  {'max_depth': 12, 'min_child_weight': 5, 'subs...          0.872029   \n",
       "15  {'max_depth': 12, 'min_child_weight': 7, 'subs...          0.860513   \n",
       "5   {'max_depth': 10, 'min_child_weight': 5, 'subs...          0.848986   \n",
       "12  {'max_depth': 12, 'min_child_weight': 5, 'subs...          0.867466   \n",
       "0   {'max_depth': 10, 'min_child_weight': 3, 'subs...          0.852378   \n",
       "1   {'max_depth': 10, 'min_child_weight': 3, 'subs...          0.853333   \n",
       "3   {'max_depth': 10, 'min_child_weight': 5, 'subs...          0.845762   \n",
       "8   {'max_depth': 10, 'min_child_weight': 7, 'subs...          0.844651   \n",
       "7   {'max_depth': 10, 'min_child_weight': 7, 'subs...          0.842428   \n",
       "4   {'max_depth': 10, 'min_child_weight': 5, 'subs...          0.847058   \n",
       "6   {'max_depth': 10, 'min_child_weight': 7, 'subs...          0.840842   \n",
       "\n",
       "    mean_test_score  \n",
       "20         0.805825  \n",
       "23         0.805320  \n",
       "21         0.805269  \n",
       "11         0.805135  \n",
       "14         0.805067  \n",
       "10         0.804764  \n",
       "26         0.804529  \n",
       "19         0.804461  \n",
       "22         0.804394  \n",
       "17         0.804209  \n",
       "18         0.804007  \n",
       "9          0.803923  \n",
       "24         0.803485  \n",
       "25         0.803266  \n",
       "2          0.802744  \n",
       "16         0.802626  \n",
       "13         0.802593  \n",
       "15         0.802559  \n",
       "5          0.802239  \n",
       "12         0.802088  \n",
       "0          0.802054  \n",
       "1          0.801768  \n",
       "3          0.801717  \n",
       "8          0.801700  \n",
       "7          0.800808  \n",
       "4          0.800724  \n",
       "6          0.799764  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameter grid\n",
    "params = {\n",
    "    'max_depth' : [10, 12, 14],\n",
    "    'min_child_weight' : [3, 5, 7],\n",
    "    'subsample' : [0.6, 0.7, 0.8] \n",
    "}\n",
    "\n",
    "# create xgboost model\n",
    "xgb_model = xgb.XGBClassifier(objective = 'multi:softmax',\n",
    "                              learning_rate = 0.1,\n",
    "                              num_classes = 3,\n",
    "                              gamma = 0.001,\n",
    "                              colsample_bytree = 0.6,\n",
    "                              colsample_bylevel = 0.6,\n",
    "                              colsample_bynode = 0.6,  \n",
    "                              seed = 27)\n",
    "\n",
    "# create grid search object\n",
    "grid_xgb = GridSearchCV(estimator = xgb_model, \n",
    "                       param_grid = params, \n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=-1,\n",
    "                       cv=5,\n",
    "                       refit = True,\n",
    "                       return_train_score = True,\n",
    "                       verbose = 1)\n",
    "\n",
    "# fit the model\n",
    "grid_xgb.fit(X_train, y_train_multinomial)\n",
    "\n",
    "# read results of grid search into dataframe\n",
    "cv_results_df = pd.DataFrame(grid_xgb.cv_results_)\n",
    "\n",
    "# print results\n",
    "cv_results_df[['params', 'mean_train_score', 'mean_test_score']].sort_values(by = ['mean_test_score'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction on test set\n",
    "y_pred = grid_xgb.best_estimator_.predict(X_test)\n",
    "\n",
    "# map back to string classes\n",
    "y_pred = pd.Series(y_pred).map(classes_inv)\n",
    "\n",
    "# create submission data frame\n",
    "y_test.loc[:,'status_group'] = y_pred\n",
    "\n",
    "# write to csv\n",
    "y_test.to_csv('submission8.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 8487\n",
       "non functional             5525\n",
       "functional needs repair     838\n",
       "Name: status_group, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
